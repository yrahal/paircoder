# Change Log

## [0.0.1]

- Ability to generate code or text using a local or a remote server.
- Integration of a container that serves llama.cpp, configurable to work with any of their supported models.

## [0.0.4]

- A new command to interrupt prediction. Starting a new prediction will also interrupt the current one.
- A new option to customize the number of predicted tokens.
- By default, Docker images are now tied to the same release number as the extension.
